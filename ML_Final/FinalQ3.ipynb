{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iter(env, gamma, theta):\n",
    "    # env.env.nS is the number of states\n",
    "    # env.env.nA is the number of actions\n",
    "    # env.env.P is the model that includes:\n",
    "    # p: probability of going from one state to another via a specific action\n",
    "    # r: reward\n",
    "    # s_: next state\n",
    "    V = np.zeros(env.env.nS) #Initial Values\n",
    "    pi = np.zeros(env.env.nS) #Initial Policy\n",
    "    count_iter = 0 # for Counting Iteration\n",
    "    while True:\n",
    "        count_iter+=1\n",
    "        delta = 0\n",
    "        for s in range(env.env.nS): #for each state compute:\n",
    "            v = V[s]\n",
    "            values = list()\n",
    "            for a in range(env.env.nA): #for each action compute\n",
    "                val = 0\n",
    "                for p, s_, r, _ in env.env.P[s][a]:\n",
    "                    val += p * (r + gamma * V[s_])#value that gets added if a specific action is taken in a specific state\n",
    "                values.append(val)\n",
    "            V[s] = max(values)#Figuring out the best value for this state\n",
    "            pi[s] = np.argmax(values)#Figuring out which action gives the best value\n",
    "            delta = max(delta, abs(v - V[s]))#Checking how much the value changes\n",
    "        if delta < theta: break#If there's not much change in value, stop\n",
    "    \n",
    "    return V, pi, count_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFFH\n",
      "FFHHF\n",
      "FFFFF\n",
      "HHFHF\n",
      "FFFFG\n"
     ]
    }
   ],
   "source": [
    "#Defining the map\n",
    "custom_map = [\n",
    "    'SFFFH',\n",
    "    'FFHHF',\n",
    "    'FFFFF',\n",
    "    'HHFHF',\n",
    "    'FFFFG'\n",
    "]\n",
    "#Using gym to create the environment\n",
    "env = gym.make('FrozenLake-v0', desc=custom_map, is_slippery=True)\n",
    "#Resetting the game\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Iteration\n",
      "[[0.00698058 0.00750863 0.00350883 0.00138496 0.        ]\n",
      " [0.01021292 0.01204922 0.         0.         0.10575782]\n",
      " [0.01383751 0.02481905 0.06172147 0.09328092 0.26750555]\n",
      " [0.         0.         0.09976105 0.         0.57087423]\n",
      " [0.10099983 0.15470592 0.29041378 0.57992437 0.        ]]\n",
      "Time:  0.03497815132141113\n",
      "Number  of Iterations:  27\n",
      "Policy:\n",
      " [['D' 'L' 'U' 'L' 'L']\n",
      " ['D' 'L' 'L' 'L' 'D']\n",
      " ['U' 'U' 'D' 'D' 'R']\n",
      " ['L' 'L' 'L' 'L' 'R']\n",
      " ['D' 'D' 'D' 'D' 'L']]\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "V, pi, countiter = value_iter(env, gamma=0.85, theta=1e-4)\n",
    "t1 = time.time()\n",
    "print('Value Iteration')\n",
    "print(V.reshape([5, 5]))\n",
    "print('Time: ', t1-t0)\n",
    "print('Number  of Iterations: ', countiter)\n",
    "actdict = {0:'L', 1:'D', 2:'R', 3:'U'}\n",
    "policy = np.array([actdict[p] for p in pi])\n",
    "print('Policy:\\n', np.array(policy).reshape([5, 5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iter(env, gamma, theta):\n",
    "    # env.env.nS is the number of states\n",
    "    # env.env.nA is the number of actions\n",
    "    # env.env.P is the model that includes:\n",
    "    # p: probability of going from one state to another via a specific action\n",
    "    # r: reward\n",
    "    # s_: next state\n",
    "    V = np.zeros(env.env.nS) #Initial Values\n",
    "    #pi = np.random.randint(4,size=env.env.nS) #Random Initial Policies\n",
    "    pi = np.ones(env.env.nS) #Initial Policies (all of them are set to one)\n",
    "    count_iter = 0# for Counting Iteration\n",
    "    while True:\n",
    "        count_iter+=1\n",
    "        #Compute Values for this Policy\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in range(env.env.nS):#for each state compute:\n",
    "                v = V[s]\n",
    "                val = 0\n",
    "                for p, s_, r, _ in env.env.P[s][pi[s]]:#compute only for action 1 (down)\n",
    "                    val += p * (r + gamma * V[s_])\n",
    "                V[s] = val\n",
    "                delta = max(delta, abs(v - V[s]))#Checking how much the value changes\n",
    "            if delta < theta: break#If there's not much change in value, stop\n",
    "        #Update Policy\n",
    "        policy_stable = True #Assuming we've already got the best policy\n",
    "        for s in range(env.env.nS):#for each state compute:\n",
    "            old_action = pi[s]\n",
    "            values = list()\n",
    "            for a in range(env.env.nA):#for each action compute:\n",
    "                val = 0\n",
    "                for p, s_, r, _ in env.env.P[s][a]:\n",
    "                    val += p * (r + gamma * V[s_])#value that gets added if a specific action is taken in a specific state\n",
    "                values.append(val)#Figuring out the best value for this state\n",
    "            pi[s] = np.argmax(values)#Figuring out which action gives the best value\n",
    "            if old_action != pi[s]: policy_stable = False #If It's not the old action keep updating\n",
    "        if policy_stable: break#If the policy has stablized, stop updating\n",
    "            \n",
    "    return V, pi, count_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFFH\n",
      "FFHHF\n",
      "FFFFF\n",
      "HHFHF\n",
      "FFFFG\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Iteration\n",
      "[[0.00688123 0.00742604 0.00345214 0.0013585  0.        ]\n",
      " [0.01015866 0.01200206 0.         0.         0.10575116]\n",
      " [0.01379926 0.02479848 0.06173264 0.09328062 0.26749831]\n",
      " [0.         0.         0.09980908 0.         0.5708703 ]\n",
      " [0.10128224 0.15490417 0.29053499 0.57997889 0.        ]]\n",
      "Time:  0.019987821578979492\n",
      "Number  of Iterations:  3\n",
      "Policy:\n",
      " [['D' 'L' 'U' 'L' 'L']\n",
      " ['D' 'L' 'L' 'L' 'D']\n",
      " ['U' 'U' 'D' 'D' 'R']\n",
      " ['L' 'L' 'L' 'L' 'R']\n",
      " ['D' 'D' 'D' 'D' 'L']]\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "V, pi, countiter = policy_iter(env, gamma=0.85, theta=1e-4)\n",
    "t1 = time.time()\n",
    "print('Policy Iteration')\n",
    "print(V.reshape([5, 5]))\n",
    "print('Time: ', t1-t0)\n",
    "print('Number  of Iterations: ', countiter)\n",
    "actdict = {0:'L', 1:'D', 2:'R', 3:'U'}\n",
    "policy = np.array([actdict[p] for p in pi])\n",
    "print('Policy:\\n', np.array(policy).reshape([5, 5]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
